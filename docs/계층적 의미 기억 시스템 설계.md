# 계층적 의미 기억 시스템(Hierarchical Semantic Memory System) 설계


**아래 설계는 초기 단계의 설계이며 현재 구현되어 있는 코드와 다를 수 있음.**

## 1. 서론

### 1.1 연구 배경

대화형 AI 시스템의 장기 기억 관리를 위한 계층적 의미 기억 시스템(HSMS)의 설계이다. 이 시스템은 사용자와의 대화를 의미적으로 분류하여 트리 구조로 저장하고, BFS기반 탐색, 유사도 비교 알고리즘을 통해 LLM기반 기억 탐색 및 저장을 실현한다.

---

## 2. 시스템 아키텍처

### 2.1 전체 구조

HSMS 시스템은 다음과 같은 파일 구조를 가진다.

주요 파일
```
hsms.py

main_ai.py
tree.py
memory.py
ai_func.py

config.py
all_memory.json
hierarchical_memory.json

```

hsms에서 시작하고 실질적인 대화, 함수 호출 등은 main_ai.py에서 실행된다. 그리고 기억 호출은 tree.py에서 진행되낟. memory.py에서는 기억 관리 함수들이 존재한다. ai_fucn.py에서는 모든 AI 함수가 존재한다.

### 2.2 hsms.py
hsms.py는 프로그램의 진입점(Entry Point) 역할을 하는 메인 실행 파일이다. 시스템 초기화, 설정 검증, 그리고 실행 모드 분기를 담당한다.

#### 2.2.1 실행 방법
```bash
python hsms.py
```
위와 같은 명령으로 시작한다.

#### 2.2.2 명령행 인자 구조
프로그램을 실행하면 아래와 같은 인자를 함께 사용할 수 있다.
```python
parser = argparse.ArgumentParser(description='계층적 의미 기억 시스템')
parser.add_argument(
	'--mode', 
	choices=['test', 'chat'], 
	default='chat',
	help='실행 모드: test (테스트), chat (대화형 모드), discord (Discord 봇 모드)
)
parser.add_argument(
	'--search', 
	choices=['efficiency', 'force', 'no'], 
	default='efficiency',
	help='검색 방법 효율(need_memory_judgement 사용), '
)
parser.add_argument(
	'--api-info',
	action='store_true',
	help='사용 가능한 API 키 정보 표시(AI, LOAD 의 수, 각 API의 첫 4자자리)'
)
parser.add_argument(
	'--tree',
	action='store_true',
	help='현재 트리 구조를 도식화하여 표시'
)
parser.add_argument(
	'--debug',
	action='store_true',
	help='디버그 모드 활성화'
)
parser.add_argument(
	'--debug-txt',
	action='store_true',
	help='디버그 텍스트 파일 저장 모드 활성화'
)
parser.add_argument(
	'--fanout-limit',
	type=int,
	default=5,
	help='한 노드가 가질 수 있는 최대 자식 수.'
)
parser.add_argument(
	'--model',
	type=str,
	default='gemini-2.5-flash',
	help='사용할 AI 모델 (gemini-1.5-flash, gemini-2.5-flash 등)'
)
parser.add_argument(
	'--no-record',
	action='store_true',
	help='대화를 기억 시스템에 저장하지 않습니다 (단순 응답 모드).'
)
parser.add_argument(
	'--update-topic',
	choices=['always', 'smart', 'never'],
	default='never',
	help='토픽 업데이트 모드: always (항상), smart (조건부), never (안함)'
)
```

#### 2.2.3 인자 상세 설명 및 동작

**mode test** : main_ai.py -> `test_mode()`함수를 실행
**mode chat** : main_ai.py -> `chat_mode()`함수를 실행
- config.json->`SYSTEM_MODE`에 저장
- 기본 값 : chat

**search efficiency** : 사용자에게 질문을 받으면 `need_memory_judgement_AI()` 함수로 기억 필요성을 판단 후 검색
**search force** : 사용자의 모든 질문에 대해 무조건 기억 검색 수행
**search no** : 기억 검색을 수행하지 않음 (순수 응답 모드)
- config.json->`SEARCH_MODE`에 저장
- 기본값 : efficiency

**api-info** : 프로그램 시작 전에 API 키 정보를 다음 형식으로 출력
```bash
========[A P I]========
AI_API   : 3개    # 주요 AI 호출용 (대용량 처리, 중요한 판단)
LOAD_API : 35개   # 병렬 처리용 (유사도 비교, 반복 작업)
------------------------
AI_API_1 : AIDF~
AI_API_2 : AICE~
AI_API_3 : AIHE~
LOAD_API_1 : LODF~
LOAD_API_2 : LOCE~
...
LOAD_API_35 : LOHE~
========================
```

**tree** : 프로그램 시작 전에 현재 메모리 트리 구조를 시각화하여 출력 (ASCII 기반 호환성 출력)
```
======================[T R E E]======================
전체 노드 수 : 7개
깊이 1 : 2개
깊이 2 : 3개
깊이 3 : 3개
-----------------------------------------------------
ROOT
+-- 카테고리1 (예: 과학기술)
    +-- 하위카테고리1-1 (예: 인공지능)
        +-- 대화노드1 [대화 0,1,2]
        +-- 대화노드2 [대화 3,5]
│   └── 하위카테고리1-2 (예: 물리학)
│       └── 대화노드3 [대화 4,6]
└── 카테고리2 (예: 일상생활)
    └── 대화노드4 [대화 7,8,9]
======================================================
```

**debug** : 디버그 모드 사용 여부
- config.json->`DEBUG`에 저장
- 기본값 : False
- 활성화 시 모든 AI 호출 과정, 검색 과정, 저장 과정에 대한 상세 로그 출력

**debug-txt** : 디버그 메시지를 텍스트 파일로 저장
- config.json->`DEBUG-TXT`에 저장
- 기본값 : False
- 활성화 시 `debug_log_YYYYMMDD_HHMMSS.txt` 파일에 모든 디버그 메시지 저장

**fanout-limit** : 한 부모 노드가 가질 수 있는 자식 노드의 최대 수
- config.json->`FANOUT-LIMIT`에 저장
- 기본값 : 5
- 이 값을 초과하면 자동으로 클러스터링 수행

**model** : gemini API에 사용할 모델
- config.json->`MODEL`에 저장
- 기본값 : gemini-2.5-flash-lite
- 지원 모델: gemini-1.5-flash, gemini-2.5-flash, gemini-2.5-flash-lite

**no-record** : 기록을 하지 않는 단순 응답 모드
- config.json->`NO-RECORD`에 저장
- 기본값 : False
- 활성화 시 대화 저장 없이 순수 응답만 수행

**update-topic** : 부모 노드의 주제 및 요약 업데이트 정책
- config.json->`UPDATE-TOPIC`에 저장
- 기본값 : never
- **always**: 항상 요약 압축 및 주제 업데이트 수행
- **smart**: MAX_SUMMARY_LENGTH 초과 시에만 압축 및 업데이트 수행
- **never**: 요약 압축 없이 단순 이어붙이기만 수행 (길이 초과 시 경고만 출력)

#### 2.2.4 초기화 및 실행 흐름
1. **환경 검증**: API 키 유효성, JSON 파일 존재 여부 확인
2. **설정 로드**: config.json에서 기존 설정 불러오기
3. **인자 처리**: 명령행 인자로 설정 덮어쓰기
4. **정보 출력**: --api-info, --tree 옵션에 따른 정보 표시
5. **모드 실행**: SYSTEM_MODE에 따라 적절한 함수 호출

인자를 받고 나서 `SYSTEM_MODE`에 맞게 main_ai.py->`chat_mode()` 또는 main_ai.py->`test_mode()`를 호출한다.

e

### 2.3 ai_func.py
이 파일에는 모든 인공지능 호출 함수를 분리하여 저장하고 있다. AI 호출의 중앙화된 관리를 통해 API 키 분배, 오류 처리, 성능 모니터링을 수행한다.

#### 2.3.1 API 분류 및 역할
- **AI_API**: 주요 시스템 흐름, 중요한 판단, 대용량 처리를 담당 (보통 3개)
- **LOAD_API**: 병렬 처리, 단순 반복 작업, 유사도 비교를 담당 (보통 30개 이상)

#### 2.3.2 기본 AI 호출 함수들


**`AI(prompt, system, history=None, fine=None, api_key=None, retries=3, debug=False)`**
- **목적**: 기본적인 gemini API 단일(동기적) 호출 함수
- **사용**: 중요한 판단, 메인 응답 생성에 사용
- **API 선택**: AI_API[0]을 기본으로 사용
- **반환값**: AI 응답 텍스트 또는 오류 메시지
- **오류 처리**: ResourceExhausted(429) 및 일반 예외 처리 포함
```python
def AI(prompt='테스트', system='지침', history=None, fine=None, api_key=None, retries=3, debug=False):
	if api_key is None:
		api_key = AI_API[0]

	call_start = time.time()
	
	genai.configure(api_key=api_key)
	model = genai.GenerativeModel(GEMINI_MODEL, system_instruction=system)

	if fine:
		ex = ''.join([f"user: {q}\nassistant: {a}\n" for q, a in fine])
		combined = f"{ex}user: {prompt}"
	else:
		his = DataManager.history_str(history if history is not None else [])
		combined = f"{his}user: {prompt}"

	attempt = 0
	while True:
		try:
			resp = model.start_chat(history=[]).send_message(combined)
			txt = resp._result.candidates[0].content.parts[0].text.strip()
			result = txt[9:].strip() if txt.lower().startswith('assistant:') else txt
			
			call_end = time.time()
			
			if debug:
				print(f"AI 호출 완료 ({call_end - call_start:.2f}초)")
			
			return result
		except ResourceExhausted as e:
			err_msg = f"[ERROR] AI API 호출이 너무 많아 오류가 발생했습니다: 429 = RPM초과 : {e}"
			print(err_msg)
			return err_msg
		except Exception as e:
			err_msg = f"[ERROR] AI API 호출 중 예외 발생: {e}"
			print(err_msg)
			return err_msg
```

**`ASYNC_AI(prompt, system, history=None, fine=None, api_key=None, retries=3, debug=False)`**
- **목적**: 기본적인 gemini 단일(비동기적) 호출 함수
- **사용**: 병렬 처리의 개별 작업 단위로 사용
- **통계**: CALL_STATS에 호출 통계 자동 기록
- **반환값**: AI 응답 텍스트 또는 오류 메시지

**`ASYNC_MULTI_AI(queries, system_prompt, history=None, fine=None, debug=False, start_debug_message="--start", end_debug_message="--end")`**
- **목적**: 여러 쿼리의 병렬(비동기적) 처리
- **API 분배**: LOAD_API를 라운드로빈 방식으로 분배 사용
- **성능 최적화**: 1개 이하 쿼리 시 AI_API[0] 사용으로 오버헤드 방지
- **디버그**: 시작/완료 메시지와 성공률 통계 제공
- **반환값**: 각 쿼리에 대한 응답 리스트
```python
from config import CALL_STATS

CALL_STATS = {
    'total_calls': 0,
    'total_time': 0,
    'parallel_calls': 0,
    'error_count': 0
}

async def ASYNC_AI(prompt, system, history=None, fine=None, api_key=None, retries=3, debug=False):
    global CALL_STATS
    CALL_STATS['total_calls'] += 1
    start_time = time.time()
    
    try:
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            None, 
            AIManager.call_ai, 
            prompt, system, history, fine, api_key, retries, debug, None
        )
        
        end_time = time.time()
        CALL_STATS['total_time'] += (end_time - start_time)
        
        return result
    except Exception as e:
        CALL_STATS['error_count'] += 1
        err_msg = f"[ERROR] 비동기 AI 호출 중 예외 발생: {e}"
        print(err_msg)
        return err_msg

async def ASYNC_MULTI_AI(queries, system_prompt, history=None, fine=None, debug=False, start_debug_message = "--start", end_debug_message = "--end""):
    global CALL_STATS
    CALL_STATS = {
        'total_calls': 0,
        'total_time': 0,
        'parallel_calls': 0,
        'error_count': 0
    }
    
    if not LOAD_API or len(queries) <= 1:
        tasks = [ASYNC_AI(q, system_prompt, history, fine, AI_API[0], debug=debug) for q in queries]
        return await asyncio.gather(*tasks)

    CALL_STATS['parallel_calls'] += 1
    start_time = time.time()

    # call_infos 및 label 관련 코드 제거됨
    
    if debug:
        print(f"병렬 AI 호출 시작 ({len(queries)}개)")

    async def run_and_debug(i, query):
        api_key = LOAD_API[i % len(LOAD_API)]
        try:
            result = await ASYNC_AI(query, system_prompt, history, fine, api_key, debug=debug)
            return result
        except Exception as e:
            err_msg = f"[ERROR] 병렬 AI 호출 중 예외 발생 (TASK-{i+1:02d}): {e}"
            print(err_msg)
            return err_msg

    tasks = [run_and_debug(i, q) for i, q in enumerate(queries)]
    
    results = await asyncio.gather(*tasks)
    
    success_count = sum(1 for r in results if r is not None and r != "")

    if debug:
        end_time = time.time()
        total_duration = end_time - start_time
        success_count = sum(1 for r in results if r is not None and r != "")
        print(f"병렬 AI 호출 완료 ({total_duration:.2f}초)(성공 {success_count}/{len(queries)})")

    return results

```


#### 2.3.3 전문 AI 함수들

**`need_memory_judgement_AI(user_input)`**
- **목적**: 사용자 질문에 대한 기억 검색 필요성 판단
- **사용 조건**: config.json->`SEARCH_MODE`가 'efficiency'일 때 사용
- **호출 위치**: main_ai.py에서 호출
- **판단 기준**:
  - **True 반환**: "저번에", "이전에", "과거에", "전에" 등의 과거 참조 표현
  - **True 반환**: "내가 이야기했던", "우리가 나눴던" 등의 이전 대화 참조
  - **True 반환**: "정리해줘", "요약해줘", "다시 보여줘" 등의 과거 정보 요청
  - **False 반환**: 일반적인 질문, 현재 시점 문제 해결, 미래 계획 질문
- **반환값**: Boolean (True/False)

```python
def need_memory_judgement_AI(user_input):
	system_prompt = """사용자의 발언에 응답하기 위해서 과거의 대화가 필요한지 판단해라.

다음 경우에는 반드시 "True"를 출력하라:
- 사용자가 "저번에", "이전에", "과거에", "전에" 등의 단어를 사용하여 과거 대화를 언급하는 경우
- 사용자가 "내가 이야기했던", "내가 말했던", "우리가 나눴던" 등의 표현으로 이전 대화를 참조하는 경우
- 사용자가 "정리해줘", "요약해줘", "다시 보여줘" 등의 표현으로 과거 정보를 요청하는 경우
- 사용자가 구체적인 과거 주제나 대화를 언급하는 경우

다음 경우에는 "False"를 출력하라:
- 일반적인 질문이나 정보 요청
- 현재 시점의 문제 해결
- 미래 계획이나 예측에 대한 질문

출력은 반드시 "True" 또는 "False"로만 하라."""

	prompt = f"사용자 발언:\n{user_input}"

	if self.debug:
		print(">>> 기억 필요성 판단 : ", end='\r')

	result = await self.ai_manager.call_ai_async_single(
		prompt, system_prompt
	)
	result = result.strip().lower()
	
	if self.debug:
		print(f">>> 기억 필요성 판단 : {result}")
	
	return result == 'true'
```
위와 같은 형태를 가진다. 간단하지만 이렇게 분리하여 관리하기 쉽게 하려고 한다.

**`respond_AI(user_input, memory=None)`**
- **목적**: 사용자 질문에 대한 최종 응답 생성
- **호출 위치**: main_ai.py에서 호출
- **입력 형식**:
  - `user_input`: 사용자의 현재 질문
  - `memory`: ALL_MEMORY.json의 인덱스 리스트 (예: [2, 4, 8])
- **응답 구성**:
  - 과거 대화가 있을 경우 다음 형식으로 컨텍스트 제공:
    ```
    ==================================[ 4 번 ]==================================
    user : 사용자가 한 말
    AI   : AI가 한 말
    ============================================================================
    ```
  - 현재 사용자 질문 추가
- **반환값**: 최종 응답 텍스트

```python
일부 슈도 코드

def respond_AI(user_input, memory=None):
	
	memory는 다음과 같은 구조를 가진다.
	memory = [2, 4, 8]
	그냥 각 대화가 저장된 ALL_MEMORY.json의 좌표가 저정되어 있다.


	system_prompt = """사용자의 발화에 응답해라. 과거 대화가 주어진다면 그 대화를 기반으로 응답해라. 간단하게 응답해라."""
	prompt = ""	
	
	if memory is not None:
		for m in memory:
			prompt += m 좌표의 ALL_MEMORY 대화를 아래 예시와 같은 형태로 넣어야함. ALL_MEMORY.json에서 원본을 그대로 가지고 와야함.
			예시:
			==================================[ 4 번 ]==================================
			user : 사용자가 한 말
			AI   : AI가 한 말
			============================================================================
	
	prompt += f"\n{user_input}"

	if self.debug:
		print(">>> 최종 응답 생성 중", end='\r')

	result = AI(
		prompt, system_prompt
	)
	
	if self.debug:
		print(f">>> 최종 응답 생성 완료")
	
	return result
```

**`judgement_similar_AI(current_conversation, node_id)`**
- **목적**: 단일 노드와 현재 대화의 유사도 판단
- **사용**: 단일 노드 검증이나 디버깅 목적
- **입력**: 현재 대화 내용, 검사할 노드 ID
- **처리 과정**: hierarchical_memory.json에서 node_id의 주제와 요약 추출
- **반환값**: 0.0~1.0 사이의 유사도 점수 (문자열)

**`judgement_similar_multi_AI(node_ids, current_conversation)`**
- **목적**: 여러 노드와 현재 대화의 유사도를 병렬로 판단
- **사용**: BFS 탐색에서 핵심적으로 사용
- **최적화**: ASYNC_MULTI_AI를 통한 병렬 처리로 성능 향상
- **입력**: 검사할 노드 ID 리스트, 현재 대화 내용
- **처리 과정**:
  1. 각 node_id에 대해 hierarchical_memory.json에서 정보 추출
  2. 병렬로 유사도 판단 쿼리 생성
  3. ASYNC_MULTI_AI로 동시 실행
- **반환값**: 각 노드에 대한 0.0~1.0 유사도 점수 리스트 (문자열)
- **노드 키**: UUID4 형식 사용

**개선된 유사도 판단 프롬프트**:
```python
system_prompt = """노드의 주제와 요약을 보고 사용자 질문과의 관련성을 0.0~1.0 사이의 점수로 평가해라.
- 0.9 이상: 매우 강한 관련성 (같은 구체적 주제)
- 0.7~0.8: 강한 관련성 (같은 카테고리의 세부 주제)
- 0.5~0.6: 보통 관련성 (같은 큰 분야이지만 다른 세부 주제)
- 0.3~0.4: 약한 관련성 (넓은 의미로만 관련)
- 0.2 이하: 관련성 없음

점수만 숫자로 출력하세요 (예: 0.85)"""

prompt = f"노드 정보 : {node_summation}\n현재 대화 : {current_conversation}\n\n위 노드와 현재 대화의 유사도를 0.0~1.0 점수로 평가하세요."
```


**`clustering_AI(candidate_node_ids, current_conversation, fanout_limit)`**
- **목적**: 클러스터링 대상 노드 선택 및 새 부모 노드 주제 생성
- **실행 조건**: 
  - 새로운 노드 추가 시 FANOUT-LIMIT 초과할 때
  - 후보 노드들이 모두 같은 부모의 자식이며 기억 노드(자식 없음)일 때
- **입력**: 
  - `candidate_node_ids`: 클러스터링 후보 노드 ID 리스트
  - `current_conversation`: 현재 저장할 대화
  - `fanout_limit`: 현재 설정된 fanout 제한값
- **선택 기준**: 현재 대화와 가장 유사한 노드들을 (fanout_limit-1)개 이하로 선택
- **반환값**: 
  - 선택된 노드 ID들의 리스트
  - 새롭게 생성될 부모 노드의 주제명
- **제약사항**: 클러스터링은 항상 동일한 부모 노드 내에서만 수행

#### 2.3.4 추가 필요 함수들

**`summary_AI(conversation_data, max_length)`**
- **목적**: 대화 데이터를 지정된 길이로 요약
- **사용**: 노드 생성, 부모 노드 업데이트 시
- **입력**: 요약할 대화 데이터, 최대 길이
- **반환값**: 요약된 텍스트

**`topic_generation_AI(summary_data)`**
- **목적**: 요약 데이터를 바탕으로 적절한 주제명 생성
- **사용**: 새 노드 생성, 클러스터링 후 부모 노드 생성 시
- **반환값**: 간결한 주제명
- **개선된 프롬프트**: 구체적이고 명확한 5-8자 명사형 주제명 생성

**개선된 주제 생성 프롬프트**:
```python
system_prompt = """주어진 요약 내용을 보고 핵심 주제를 파악하여 간결한 주제명을 생성해라.
주제명은 다음 규칙을 따라라:
1. 5-8자의 명사형 구문으로 작성
2. 구체적이고 명확한 내용으로 작성 (예: "과학 상식", "수학 공식", "개인 정보")
3. 너무 추상적이거나 포괄적이지 않게 작성
4. 마크다운 형식 사용 금지
5. 한국어로 작성

예시:
- "물리학 기초 개념" (O)
- "화학 원소 정보" (O)  
- "수학 정리 설명" (O)
- "일반적인 대화" (X - 너무 포괄적)
- "인사말" (X - 너무 포괄적)

주제명만 출력하세요."""
```

**`parent_update_AI(old_summary, new_content, max_length)`**
- **목적**: 부모 노드의 요약을 압축하여 업데이트
- **사용**: UPDATE_TOPIC이 'smart' 또는 'always'일 때
- **입력**: 기존 요약, 새 내용, 최대 길이
- **반환값**: 압축된 새 요약과 업데이트된 주제명


### 2.4 main_ai.py
main_ai.py는 시스템의 핵심 제어 흐름을 관리하는 파일이다. 사용자 인터페이스, 명령어 처리, 그리고 전체적인 대화 흐름 조정을 담당한다.

#### 2.4.1 전체 흐름 구조
**입력 → 기억 필요성 판단 → 기억 검색 → 응답 생성 → 기억 저장**

#### 2.4.2 실시간 명령어 시스템
chat 모드 실행 중에 사용자가 '!'로 시작하는 명령어를 입력하여 시스템 설정을 동적으로 변경할 수 있다.
**`command(command_input)`**
실시간 명령어 처리 함수로, 다음 명령어들을 지원한다:
- **!help** : 사용 가능한 모든 명령어 목록과 사용법 표시
- **!api-info** : 현재 사용 가능한 API 키 개수와 상태 확인
- **!status** : 현재 시스템 모드 및 설정 상태 표시
- **!search [MODE]** : 검색 모드 변경 (efficiency/force/no)
- **!debug** : 디버그 모드 토글 (활성화/비활성화)
- **!debug-txt** : 디버그 텍스트 파일 저장 모드 토글
- **!fanout-limit [NUMBER]** : fanout 제한값 변경 (1-50 범위)
- **!model [MODEL_NAME]** : 사용할 gemini 모델 변경
- **!no-record** : 기록 모드 토글 (저장/비저장)
- **!update-topic [MODE]** : 토픽 업데이트 정책 변경 (always/smart/never)
- **!max-summary-length [NUMBER]** : 요약 최대 길이 설정

각 명령어 실행 후 변경된 설정을 사용자에게 확인 메시지로 출력한다.

**`main(user_question)`**
핵심 대화 처리 로직을 담당한다:
```python
def main(user_question):
    # 1. 기억 필요성 판단
    need_memory = False
    if SEARCH_MODE == 'efficiency':
        need_memory = need_memory_judgement_AI(user_question)
    elif SEARCH_MODE == 'force':
        need_memory = True
    # SEARCH_MODE == 'no'인 경우 need_memory는 False 유지
    
    # 2. 기억 검색 (필요한 경우)
    memory_results = []
    if need_memory:
        memory_results = search_tree(user_question)  # ALL_MEMORY 인덱스 리스트 반환
    
    # 3. 응답 생성
    response = respond_AI(user_question, memory_results)
    
    # 4. 기억 저장 (NO_RECORD가 False인 경우)
    if not NO_RECORD:
        conversation_pair = [
            {"role": "user", "content": user_question},
            {"role": "assistant", "content": response}
        ]
        save_tree(conversation_pair)
    
    return response
```

**`chat_mode()`**
터미널 기반 대화형 모드:
```python
def chat_mode():
    print("=== HSMS 대화 모드 시작 ===")
    print("'!help'로 명령어 확인, 'exit'로 종료")
    
    while True:
        user_input = input("사용자: ").strip()
        
        if user_input.lower() == 'exit':
            print("시스템을 종료합니다.")
            break
            
        if user_input.startswith('!'):
            command(user_input)
            continue
        
        if not user_input:  # 빈 입력 처리
            continue
            
        try:
            response = main(user_input)
            print(f"AI: {response}")
        except Exception as e:
            print(f"오류 발생: {e}")
```

**`test_mode()`**
자동화된 테스트 모드:
```python
def test_mode():
    print("=== HSMS 테스트 모드 시작 ===")
    
    for i, test_question in enumerate(TEST_Q, 1):
        print(f"\n[테스트 {i}/{len(TEST_Q)}]")
        print(f"질문: {test_question}")
        
        try:
            response = main(test_question)
            print(f"응답: {response}")
        except Exception as e:
            print(f"오류: {e}")
        
        if DEBUG:
            input("다음 테스트로 진행하려면 Enter를 누르세요...")
    
    print("\n=== 테스트 완료 ===")
```

#### 2.4.3 디버그 시스템

**디버그 메시지 형식**
모든 디버그 메시지는 다음과 같은 일관된 형식을 따른다:
```
>>> [작업명] : [상태] | [2025.09.03-21:21:30]
```

**디버그 메시지 특징**:
- 결과 중심 출력: 시작과 완료를 명확히 표시
- 시간 정보: 모든 메시지 우측에 `[YYYY.MM.DD-HH:MM:SS]` 형식 타임스탬프
- 진행 상황: `\r`을 사용한 실시간 상태 업데이트

**DEBUG-TXT 모드**:
- 디버그 메시지가 활성화된 시점부터 모든 디버그 출력을 텍스트 파일에 저장
- 파일명: `debug_log_YYYYMMDD_HHMMSS.txt`
- 실시간 저장으로 프로그램 중단 시에도 로그 보존

**디버그 관리 함수** (config.py):
- `debug_print(message, end='\n')`: 타임스탬프가 포함된 디버그 메시지 출력
- `debug_log_init()`: 디버그 로그 파일 초기화
- `get_timestamp()`: 현재 시간을 지정된 형식으로 반환


### 2.5 tree.py
tree.py는 계층적 메모리 트리의 검색과 저장을 담당하는 핵심 모듈이다. BFS 기반 검색 알고리즘과 동적 클러스터링을 통한 트리 구조 최적화를 수행한다.

#### 2.5.1 검색 시스템

**`search_tree(current_conversation)`**
BFS(너비 우선 탐색) 방식으로 관련 기억을 탐색하는 함수:

```python
async def search_tree(current_conversation):
    # 초기화
    current_level_nodes = get_root_children_ids()  # ROOT의 직접 자식들
    visited = set()
    found_memories = []
    depth = 1
    
    if DEBUG:
        debug_print(f"BFS 검색 시작 (초기 노드: {len(current_level_nodes)}개)")
    
    while current_level_nodes:
        if DEBUG:
            debug_print(f"깊이 {depth} 탐색 중 ({len(current_level_nodes)}개 노드)")
        
        # 현재 레벨의 모든 노드에 대해 병렬 유사도 검사
        similarity_results = await judgement_similar_multi_AI(
            current_level_nodes, current_conversation
        )
        
        next_level_nodes = []
        
        for i, node_id in enumerate(current_level_nodes):
            if node_id in visited:
                continue
                
            visited.add(node_id)
            
            if similarity_results[i].strip().lower() == 'true':
                node_data = get_node_data(node_id)
                
                # 기억 노드인 경우 (자식이 없는 경우)
                if not node_data['children_ids']:
                    found_memories.extend(node_data['all_memory_indexes'])
                    if DEBUG:
                        debug_print(f"기억 발견: 노드 {node_id[:8]}... ({len(node_data['all_memory_indexes'])}개 대화)")
                
                # 부모 노드인 경우 (자식이 있는 경우)
                else:
                    next_level_nodes.extend(node_data['children_ids'])
                    if DEBUG:
                        debug_print(f"하위 탐색: 노드 {node_id[:8]}... ({len(node_data['children_ids'])}개 자식)")
        
        current_level_nodes = next_level_nodes
        depth += 1
        
        # 무한 루프 방지 (최대 깊이 제한)
        if depth > MAX_SEARCH_DEPTH:
            if DEBUG:
                debug_print(f"최대 검색 깊이 도달 ({MAX_SEARCH_DEPTH})")
            break
    
    if DEBUG:
        debug_print(f"BFS 검색 완료 (발견된 기억: {len(found_memories)}개)")
    
    return found_memories
```

**검색 최적화 특징**:
- **병렬 처리**: 같은 깊이의 모든 노드를 동시에 평가
- **중복 방지**: visited set을 통한 노드 재방문 방지
- **깊이 제한**: 무한 루프 방지를 위한 최대 깊이 설정
- **실시간 피드백**: 디버그 모드에서 탐색 진행상황 실시간 표시

#### 2.5.2 저장 시스템

**`save_tree(conversation_pair)`**
새로운 대화를 적절한 위치에 저장하고 필요시 클러스터링을 수행:

```python
async def save_tree(conversation_pair):
    if DEBUG:
        debug_print("대화 저장 프로세스 시작")
    
    # 1. 대화를 ALL_MEMORY에 추가하고 인덱스 획득
    memory_index = add_to_all_memory(conversation_pair)
    
    # 2. 대화 요약 생성
    conversation_summary = summary_AI(conversation_pair, MAX_SUMMARY_LENGTH)
    
    # 3. 저장 위치 탐색 (단일 경로 탐색)
    target_location = find_storage_location(conversation_pair)
    
    # 4. 기존 기억 노드에 추가 vs 새 노드 생성 결정
    if target_location['existing_memory_node']:
        # 기존 기억 노드에 추가
        await add_to_existing_node(target_location['node_id'], memory_index, conversation_summary)
    else:
        # 새 기억 노드 생성 필요
        parent_id = target_location['parent_id']
        
        # Fanout 제한 확인
        if will_exceed_fanout_limit(parent_id):
            # 클러스터링 수행
            await perform_clustering(parent_id, memory_index, conversation_summary)
        else:
            # 직접 새 노드 생성
            await create_new_memory_node(parent_id, memory_index, conversation_summary)
    
    if DEBUG:
        debug_print("대화 저장 프로세스 완료")
```

**`find_storage_location(conversation_pair)`**
저장할 위치를 탐색하는 함수 (search_tree와 유사하지만 단일 경로):

```python
def find_storage_location(conversation_pair):
    current_node_id = "ROOT"
    depth = 0
    
    while depth < MAX_STORAGE_DEPTH:
        children = get_children_ids(current_node_id)
        
        if not children:
            # 리프 노드 도달
            return {
                'parent_id': current_node_id,
                'existing_memory_node': False
            }
        
        # 가장 유사한 자식 노드 찾기
        best_match = find_best_matching_child(children, conversation_pair)
        
        if best_match['is_memory_node'] and best_match['similarity_score'] > SIMILARITY_THRESHOLD:
            # 기존 기억 노드에 추가
            return {
                'node_id': best_match['node_id'],
                'existing_memory_node': True
            }
        elif best_match['similarity_score'] > EXPLORATION_THRESHOLD:
            # 더 깊이 탐색
            current_node_id = best_match['node_id']
            depth += 1
        else:
            # 유사한 노드 없음, 현재 위치에 새 노드 생성
            return {
                'parent_id': current_node_id,
                'existing_memory_node': False
            }
    
    # 최대 깊이 도달
    return {
        'parent_id': current_node_id,
        'existing_memory_node': False
    }
```

#### 2.5.3 클러스터링 시스템

**`perform_clustering(parent_id, new_memory_index, new_conversation_summary)`**
Fanout 제한 초과 시 자동으로 수행되는 클러스터링:

```python
async def perform_clustering(parent_id, new_memory_index, new_conversation_summary):
    if DEBUG:
        debug_print(f"클러스터링 시작 (부모: {parent_id[:8]}...)")
    
    # 1. 현재 부모의 모든 자식 노드 (기억 노드만) 획득
    children_ids = get_memory_children_ids(parent_id)  # 자식이 없는 노드들만
    
    # 2. 클러스터링 대상 선택
    clustering_result = await clustering_AI(
        children_ids, 
        new_conversation_summary, 
        FANOUT_LIMIT
    )
    
    selected_node_ids = clustering_result['selected_nodes']
    new_parent_topic = clustering_result['new_topic']
    
    # 3. 새로운 중간 부모 노드 생성
    new_parent_id = create_uuid()
    
    # 4. 선택된 노드들의 데이터 수집
    selected_nodes_data = []
    all_memory_indexes = [new_memory_index]  # 새 대화 포함
    
    for node_id in selected_node_ids:
        node_data = get_node_data(node_id)
        selected_nodes_data.append(node_data)
        all_memory_indexes.extend(node_data['all_memory_indexes'])
    
    # 5. 새 부모 노드 요약 생성
    combined_summary = generate_combined_summary(selected_nodes_data, new_conversation_summary)
    
    # 6. 새 부모 노드 생성
    new_parent_node = {
        'node_id': new_parent_id,
        'topic': new_parent_topic,
        'summary': combined_summary,
        'direct_parent_id': parent_id,
        'all_parent_ids': get_node_data(parent_id)['all_parent_ids'] + [parent_id],
        'children_ids': selected_node_ids + [create_uuid()],  # 기존 + 새 노드
        'all_memory_indexes': []  # 부모 노드는 직접 기억 보관하지 않음
    }
    
    # 7. 새 기억 노드 생성 (현재 대화용)
    new_memory_node = create_memory_node(
        new_memory_index, 
        new_conversation_summary, 
        new_parent_id
    )
    
    # 8. 트리 구조 업데이트
    update_tree_structure(parent_id, selected_node_ids, new_parent_node, new_memory_node)
    
    # 9. 부모 노드들 업데이트 (UPDATE_TOPIC 설정에 따라)
    await update_parent_nodes(new_parent_id, new_conversation_summary)
    
    if DEBUG:
        debug_print(f"클러스터링 완료 (새 부모: {new_parent_topic})")
```

**클러스터링 제약사항**:
- 동일한 부모 노드의 자식들만 대상
- 기억 노드(자식이 없는 노드)만 클러스터링 가능
- 최대 (FANOUT_LIMIT - 1)개 노드까지 선택 가능
- 현재 저장할 대화와의 유사성을 우선 고려

#### 2.5.4 부모 노드 업데이트 시스템

**`update_parent_nodes(updated_node_id, new_content_summary)`**
부모 노드들의 요약과 주제를 업데이트:

```python
async def update_parent_nodes(updated_node_id, new_content_summary):
    node_data = get_node_data(updated_node_id)
    all_parent_ids = node_data['all_parent_ids']
    
    if not all_parent_ids:
        return  # ROOT 노드인 경우
    
    # 1. 요약 이어붙이기 (순차적 처리)
    need_compression = []
    
    for parent_id in all_parent_ids:
        parent_data = get_node_data(parent_id)
        parent_data['summary'] += f"\n{new_content_summary}"
        
        # 길이 체크
        if len(parent_data['summary']) > MAX_SUMMARY_LENGTH:
            if UPDATE_TOPIC in ['smart', 'always']:
                need_compression.append(parent_id)
            elif UPDATE_TOPIC == 'never':
                debug_print(f"경고: 노드 {parent_id[:8]}... 요약 길이 초과 ({len(parent_data['summary'])}/{MAX_SUMMARY_LENGTH})")
        
        save_node_data(parent_id, parent_data)
    
    # 2. 병렬 압축 처리 (필요한 경우)
    if need_compression:
        if DEBUG:
            debug_print(f"부모 노드 압축 시작 ({len(need_compression)}개)")
        
        compression_tasks = []
        for parent_id in need_compression:
            parent_data = get_node_data(parent_id)
            task = parent_update_AI(
                parent_data['summary'], 
                MAX_SUMMARY_LENGTH
            )
            compression_tasks.append((parent_id, task))
        
        # 병렬 실행
        compression_results = await asyncio.gather(*[task for _, task in compression_tasks])
        
        # 결과 적용
        for i, (parent_id, _) in enumerate(compression_tasks):
            new_summary, new_topic = compression_results[i]
            parent_data = get_node_data(parent_id)
            parent_data['summary'] = new_summary
            if new_topic:  # 주제가 제공된 경우
                parent_data['topic'] = new_topic
            save_node_data(parent_id, parent_data)
        
        if DEBUG:
            debug_print(f"부모 노드 압축 완료 ({len(need_compression)}개)")
```

#### 2.5.5 노드 타입 및 구조

**노드 분류**:
1. **ROOT 노드**: 트리의 최상위 노드, 모든 카테고리의 부모
2. **부모 노드**: 1개 이상의 자식을 가진 중간 계층 노드, 직접적인 기억 저장 안함
3. **기억 노드**: 실제 대화를 저장하는 리프 노드, 자식 없음

**좌표 시스템**:
- `all_memory_indexes`: ALL_MEMORY.json의 배열 인덱스를 저장
- 예: `[2, 4, 8]`은 ALL_MEMORY[2], ALL_MEMORY[4], ALL_MEMORY[8]의 대화들을 의미
- 각 인덱스는 하나의 완전한 사용자-AI 대화 쌍을 나타냄
#### 2.5.6 트리 구조 진화 과정 예시

다음은 fanout-limit=3인 환경에서 트리가 점진적으로 구축되는 과정이다:

**1단계: 초기 상태**
```
ROOT [깊이 0]
```

**2단계: 첫 번째 대화 (AAAB 주제)**
```
저장 과정:
- 입력: AAAB 관련 대화
- 응답 생성 완료
- 저장 위치 탐색: ROOT에서 시작, 유사한 자식 없음
- 새 기억 노드 생성: AAAB 노드
```
```
ROOT [깊이 0]
└── AAAB [깊이 1, 대화 0] ←새로 생성
```

**3-4단계: 추가 대화들**
```
ROOT [깊이 0]
├── AAAB [깊이 1, 대화 0]
├── AAAC [깊이 1, 대화 1] ←새로 생성
└── BA [깊이 1, 대화 2] ←새로 생성
```

**5단계: 기존 노드에 대화 추가**
```
입력: AAAC와 유사한 대화
저장 과정:
- 저장 위치 탐색: AAAC 노드가 유사성 임계값 초과
- 기존 AAAC 노드에 추가
```
```
ROOT [깊이 0]
├── AAAB [깊이 1, 대화 0]
├── AAAC [깊이 1, 대화 1, 4] ←업데이트
└── BA [깊이 1, 대화 2]
```

**6단계: Fanout 제한으로 인한 첫 번째 클러스터링**
```
입력: BB 관련 대화
문제: ROOT에 새 노드 추가 시 fanout-limit(3) 초과
해결: 클러스터링 수행

클러스터링 분석:
- 후보 노드: [AAAB, AAAC, BA]
- clustering_AI 결과: BA와 BB를 "B" 주제로 클러스터링
- 새 부모 노드 "B" 생성
```
```
ROOT [깊이 0]
├── AAAB [깊이 1, 대화 0]
├── AAAC [깊이 1, 대화 1, 4]
└── B [깊이 1] ←새 부모 노드
    ├── BA [깊이 2, 대화 2]
    └── BB [깊이 2, 대화 5] ←새로 생성
```

**7-8단계: 계속된 클러스터링**
```
입력: AB 관련 대화
문제: 다시 fanout-limit 초과
해결: AAAB, AAAC를 "A" 주제로 클러스터링

입력: AAAA 관련 대화  
문제: A 노드에서 fanout-limit 초과
해결: AAAB, AAAC를 "AAA" 주제로 클러스터링
```

**최종 구조 (10단계 후)**
```
ROOT [깊이 0]
├── A [깊이 1]
│   ├── AB [깊이 2, 대화 6]
│   ├── AC [깊이 2, 대화 8]
│   └── AA [깊이 2]
│       ├── AAB [깊이 3, 대화 9]
│       └── AAA [깊이 3]
│           ├── AAAA [깊이 4, 대화 7]
│           ├── AAAB [깊이 4, 대화 0]
│           └── AAAC [깊이 4, 대화 1, 4]
└── B [깊이 1]
    ├── BA [깊이 2, 대화 2]
    └── BB [깊이 2, 대화 5]
```

**핵심 특징**:
- **동적 구조**: 대화 추가에 따라 트리가 자동으로 재구성
- **유사성 기반**: AI가 주제 유사성을 판단하여 클러스터링 결정
- **균형 유지**: fanout-limit을 통한 트리 깊이와 폭의 균형
- **확장성**: 무제한 대화 저장 가능한 구조

#### 2.5.7 저장 과정 세부 알고리즘

**최종 탐색 위치 결정**:
저장을 위한 탐색은 검색과 달리 단일 경로를 따라 진행된다:

1. **깊이별 단일 선택**: 각 깊이에서 가장 유사한 노드 하나만 선택
2. **임계값 기반 결정**: 
   - 유사도 > STORAGE_THRESHOLD → 해당 노드에 저장
   - 유사도 > EXPLORATION_THRESHOLD → 더 깊이 탐색
   - 유사도 < EXPLORATION_THRESHOLD → 현재 위치에 새 노드 생성
3. **기억 노드 우선**: 기억 노드를 만나면 추가 저장 고려, 부모 노드는 통과점

**부모 노드 업데이트 정책**:
- **always**: 모든 업데이트 시 요약 압축 수행 (높은 정확도, 많은 API 호출)
- **smart**: 길이 초과 시에만 압축 (균형적 접근)
- **never**: 단순 이어붙이기만 수행 (빠름, 길어질 수 있음)


### 2.6 memory.py
memory.py는 JSON 파일 기반의 영구 저장소 관리를 담당한다. 대화 데이터와 트리 구조의 안전한 저장 및 로드를 보장한다.

#### 2.6.1 기본 파일 관리 함수

**`save_json(file_path, data, backup=True)`**
- **목적**: 딕셔너리 데이터를 JSON 파일로 안전하게 저장
- **매개변수**:
  - `file_path`: 저장할 파일 경로
  - `data`: 저장할 딕셔너리 데이터
  - `backup`: 기존 파일 백업 여부 (기본값: True)
- **안전 장치**: 
  - 저장 전 기존 파일 백업 생성
  - 원자적 쓰기 (임시 파일 → 이름 변경)
  - JSON 직렬화 오류 처리

**`load_json(file_path, default=None)`**
- **목적**: JSON 파일을 안전하게 로드하여 딕셔너리로 반환
- **매개변수**:
  - `file_path`: 로드할 파일 경로
  - `default`: 파일이 없을 때 반환할 기본값
- **오류 처리**: 
  - 파일 없음, JSON 파싱 오류 등 예외 상황 처리
  - 백업 파일 자동 복구 기능

**`update_all_memory(new_conversation)`**
- **목적**: ALL_MEMORY.json에 새로운 대화 추가
- **매개변수**: `new_conversation` - 사용자-AI 대화 쌍
- **반환값**: 추가된 대화의 인덱스 (int)
- **처리 과정**:
  1. 기존 ALL_MEMORY 로드
  2. 새 대화 append
  3. 파일 저장
  4. 인덱스 반환

#### 2.6.2 트리 구조 관리 함수

**`get_node_data(node_id)`**
- **목적**: 특정 노드의 전체 데이터 반환
- **매개변수**: `node_id` - UUID4 형식의 노드 식별자
- **반환값**: 노드 데이터 딕셔너리 또는 None

**`save_node_data(node_id, node_data)`**
- **목적**: 특정 노드의 데이터 업데이트
- **매개변수**: 
  - `node_id`: 업데이트할 노드 ID
  - `node_data`: 새로운 노드 데이터

**`create_new_node(topic, summary, parent_id, memory_indexes=None)`**
- **목적**: 새로운 노드 생성 및 트리에 추가
- **매개변수**:
  - `topic`: 노드 주제명
  - `summary`: 노드 요약
  - `parent_id`: 부모 노드 ID (ROOT인 경우 None)
  - `memory_indexes`: 기억 노드인 경우 대화 인덱스 리스트
- **반환값**: 생성된 노드의 UUID

#### 2.6.3 데이터 구조 예시

**ALL_MEMORY.json 구조**:
각 요소는 완전한 사용자-AI 대화 쌍을 나타냄
```json
[
    [
        {
            "role": "user",
            "content": "지구의 위성은 무엇인가?"
        },
        {
            "role": "assistant",
            "content": "지구의 위성은 바로 달입니다. 우리 밤하늘에서 가장 밝게 빛나는 천체이기도 하죠."
        }
    ],
    [
        {
            "role": "user",
            "content": "물의 화학식은?"
        },
        {
            "role": "assistant",
            "content": "물의 화학식은 H₂O입니다."
        }
    ]
]
```

**HIERARCHICAL_MEMORY.json 구조**:
딕셔너리 형태로 노드 ID를 키로 사용
```json
{
    "a233e573-da9f-4dbf-9ef4-3b42169c77e5": {
        "node_id": "a233e573-da9f-4dbf-9ef4-3b42169c77e5",
        "topic": "과학기술",
        "summary": "인공지능과 물리학에 대한 대화들을 포함하며, 특히 딥러닝과 양자역학에 대한 논의가 주를 이룸",
        "direct_parent_id": null,
        "all_parent_ids": [],
        "children_ids": [
            "b344f684-eb0f-5dcf-0fg5-4c53270d88f6",
            "c455g795-fc1f-6edg-1gh6-5d64381e99g7"
        ],
        "all_memory_indexes": []
    },
    "d566h806-gd2g-7feh-2hi7-6e75492f00h8": {
        "node_id": "d566h806-gd2g-7feh-2hi7-6e75492f00h8",
        "topic": "인공지능 기초",
        "summary": "머신러닝과 딥러닝의 기본 개념에 대한 질문과 답변",
        "direct_parent_id": "a233e573-da9f-4dbf-9ef4-3b42169c77e5",
        "all_parent_ids": ["a233e573-da9f-4dbf-9ef4-3b42169c77e5"],
        "children_ids": [],
        "all_memory_indexes": [0, 3, 7]
    }
}
```

#### 2.6.4 백업 및 복구 시스템

**자동 백업**:
- 모든 파일 저장 시 `.backup` 확장자로 이전 버전 보관
- 날짜별 백업 폴더 생성 (`backups/YYYYMMDD/`)
- 설정 가능한 백업 보관 기간

**데이터 무결성 검증**:
- JSON 파일 로드 시 구조 검증
- 노드 간 참조 일관성 확인
- 손상된 데이터 자동 복구 시도


### 2.7 config.py
config.py는 시스템 설정, API 키 관리, 그리고 유틸리티 함수들을 제공하는 설정 관리 모듈이다.

#### 2.7.1 API 키 관리
환경 변수에서 API 키를 자동으로 로드하고 검증한다:

```python
# AI_API 로드 (주요 AI 호출용)
AI_API = []
AI_API_N = 0
while True:
    key = os.getenv(f'AI_{AI_API_N+1}')
    if not key:
        break
    AI_API.append(key)
    AI_API_N += 1

# LOAD_API 로드 (병렬 처리용)
LOAD_API = []
LOAD_API_N = 0
while True:
    key = os.getenv(f'LOAD_{LOAD_API_N+1}')
    if not key:
        break
    LOAD_API.append(key)
    LOAD_API_N += 1

# 오류 처리 및 폴백
if AI_API_N == 0 and LOAD_API_N == 0:
    print("ERROR: NO API KEYS FOUND")
    sys.exit(1)

# 부족한 API 보완
if LOAD_API_N == 0:
    print("WARNING: NO LOAD_API, USING AI_API")
    LOAD_API = AI_API.copy()

if AI_API_N == 0:
    print("WARNING: NO AI_API, USING LOAD_API")
    AI_API = LOAD_API[:3]  # 최대 3개만 사용
```

#### 2.7.2 테스트 데이터
```python
TEST_Q = [
    "지구의 위성은 무엇인가?",
    "물의 화학식은?",
    "태양계에서 가장 큰 행성은?",
    "광합성에 필요한 빛의 색은?",
    "피타고라스의 정리를 설명하라.",
    "내 이름은 김철수이다."
    "DNA의 이중나선 구조를 발견한 과학자는?",
    "빛의 속도는 초속 몇 미터인가?",
    "뉴턴의 제1법칙을 설명하라.",
    "수소 원자의 원자번호는?",
    "에베레스트산의 높이는 약 몇 미터인가?",
    "지구의 자전주기는 몇 시간인가?",
    "나는 19살이다.",
    "물은 몇 도에서 끓는가?",
    "원소기호 O는 무엇을 의미하는가?",
    "전자기파의 예를 하나 들어라.",
    "나는 인공지능을 좋아한다.",
    "지구의 대기는 주로 어떤 기체로 이루어져 있는가?",
    "피보나치 수열의 첫 5개 숫자를 써라.",
    "내가 좋아하는 인공지능은 GEMINI이다.",
    "달의 뒷면을 처음 본 인류는 누구인가?"
]
```

#### 2.7.3 성능 모니터링
```python
CALL_STATS = {
    'total_calls': 0,
    'total_time': 0,
    'parallel_calls': 0,
    'error_count': 0,
    'memory_searches': 0,
    'cache_hits': 0
}
```

#### 2.7.4 디버그 관리 함수

**`debug_print(message, end='\n')`**
- **목적**: 타임스탬프가 포함된 일관된 디버그 메시지 출력
- **기능**: 
  - 자동 타임스탬프 추가
  - DEBUG-TXT 모드 시 파일 저장
  - 실시간 진행 상황 표시 (`end='\r'` 지원)

**`debug_log_init()`**
- **목적**: 디버그 로그 파일 초기화
- **기능**: 
  - 로그 파일명 생성 (`debug_log_YYYYMMDD_HHMMSS.txt`)
  - 로그 디렉터리 생성
  - 파일 핸들러 설정

**`get_timestamp()`**
- **목적**: 일관된 형식의 타임스탬프 반환
- **반환값**: `[YYYY.MM.DD-HH:MM:SS]` 형식 문자열

#### 2.7.5 시스템 상수
```python
# 검색 및 저장 관련
MAX_SEARCH_DEPTH = 10
MAX_STORAGE_DEPTH = 8
SIMILARITY_THRESHOLD = 0.7
EXPLORATION_THRESHOLD = 0.5

# 기본 설정값
DEFAULT_FANOUT_LIMIT = 5
DEFAULT_MAX_SUMMARY_LENGTH = 2000
DEFAULT_MODEL = "gemini-2.5-flash-lite"

# 파일 경로
ALL_MEMORY_PATH = "all_memory.json"
HIERARCHICAL_MEMORY_PATH = "hierarchical_memory.json"
CONFIG_PATH = "config.json"
```

---

## 4. 시스템 워크플로우

### 4.1 전체 시스템 흐름도

```
[사용자 입력] 
    ↓
[명령어 체크] → [명령어 처리] → [설정 업데이트]
    ↓ (일반 질문)
[기억 필요성 판단] (SEARCH_MODE에 따라)
    ↓
[기억 검색] (필요시) → [BFS 탐색] → [관련 대화 수집]
    ↓
[응답 생성] ← [과거 대화 컨텍스트]
    ↓
[대화 저장] (NO_RECORD가 False인 경우)
    ↓
[저장 위치 탐색] → [기존 노드 vs 새 노드]
    ↓
[클러스터링] (fanout 초과 시) → [트리 재구성]
    ↓
[부모 노드 업데이트] → [요약 압축] (UPDATE_TOPIC에 따라)
    ↓
[응답 출력]
```

### 4.2 기억 검색 워크플로우

#### 4.2.1 검색 트리거 결정
```python
# SEARCH_MODE별 동작
if SEARCH_MODE == 'efficiency':
    need_search = await need_memory_judgement_AI(user_input)
elif SEARCH_MODE == 'force':
    need_search = True
else:  # 'no'
    need_search = False
```

#### 4.2.2 BFS 검색 과정
```
1. 초기화: ROOT의 직접 자식들을 큐에 추가
2. 현재 레벨 처리:
   - 모든 노드에 대해 병렬 유사도 검사
   - 유사한 노드들 식별
3. 노드 타입별 처리:
   - 기억 노드: 대화 인덱스 수집
   - 부모 노드: 자식들을 다음 레벨 큐에 추가
4. 다음 깊이로 이동
5. 최대 깊이 도달 또는 큐 비움까지 반복
```

### 4.3 대화 저장 워크플로우

#### 4.3.1 저장 위치 결정
```
1. ALL_MEMORY에 대화 추가 → 인덱스 획득
2. 대화 요약 생성
3. 저장 위치 탐색 (단일 경로):
   - ROOT에서 시작
   - 각 깊이에서 가장 유사한 자식 선택
   - 임계값 기반 결정:
     * > SIMILARITY_THRESHOLD: 기존 노드에 추가
     * > EXPLORATION_THRESHOLD: 더 깊이 탐색
     * < EXPLORATION_THRESHOLD: 새 노드 생성
```

#### 4.3.2 클러스터링 트리거
```
새 노드 생성 필요 AND 부모의 자식 수 >= FANOUT_LIMIT
    ↓
클러스터링 수행:
1. 같은 부모의 기억 노드들 식별
2. clustering_AI로 그룹 선택
3. 새 중간 부모 노드 생성
4. 트리 구조 재배치
5. 모든 관련 노드의 참조 업데이트
```

### 4.4 부모 노드 업데이트 워크플로우

#### 4.4.1 순차적 요약 업데이트
```python
for parent_id in all_parent_ids:
    parent_summary += new_conversation_summary
    if len(parent_summary) > MAX_SUMMARY_LENGTH:
        if UPDATE_TOPIC in ['smart', 'always']:
            compression_needed.append(parent_id)
```

#### 4.4.2 병렬 압축 처리
```python
# 압축이 필요한 노드들을 병렬로 처리
compression_tasks = [
    parent_update_AI(node_summary, MAX_SUMMARY_LENGTH) 
    for node_id in compression_needed
]

results = await asyncio.gather(*compression_tasks)

# 결과 적용
for node_id, (new_summary, new_topic) in zip(compression_needed, results):
    update_node(node_id, new_summary, new_topic)
```

### 4.5 오류 처리 및 복구

#### 4.5.1 API 오류 처리
- **429 (Rate Limit)**: 자동 대기 후 재시도
- **일반 오류**: 다른 API 키로 폴백
- **전체 실패**: 오류 메시지 반환, 시스템 계속 동작

#### 4.5.2 데이터 무결성 보장
- 모든 파일 쓰기 전 백업 생성
- 원자적 파일 쓰기 (임시 파일 → 이름 변경)
- 로드 시 구조 검증 및 자동 복구

### 4.6 성능 최적화 전략

#### 4.6.1 병렬 처리 최적화
- 유사도 검사의 병렬 실행
- 부모 노드 업데이트의 병렬 압축
- API 키 풀을 통한 동시 요청 분산

#### 4.6.2 메모리 및 저장소 최적화
- 필요시에만 JSON 파일 로드
- 변경된 노드만 선택적 저장
- 백그라운드 백업 및 정리

---

## 3. 변수 및 함수
### 3.1 전역 변수 및 설정

#### 3.1.1 config.py 변수
**API 관리**:
- `AI_API` (list): 주요 AI 호출용 API 키 리스트 (보통 3개)
- `LOAD_API` (list): 병렬 처리용 API 키 리스트 (보통 30개 이상)
- `AI_API_N` (int): AI_API 개수
- `LOAD_API_N` (int): LOAD_API 개수

**테스트 및 모니터링**:
- `TEST_Q` (list): 테스트 모드에서 사용할 질문 리스트
- `CALL_STATS` (dict): API 호출 통계 정보
  - `total_calls`: 총 호출 수
  - `total_time`: 총 소요 시간
  - `parallel_calls`: 병렬 호출 수
  - `error_count`: 오류 발생 수

**시스템 상수**:
- `MAX_SEARCH_DEPTH` (int): 최대 검색 깊이 (기본값: 10)
- `MAX_STORAGE_DEPTH` (int): 최대 저장 탐색 깊이 (기본값: 8)
- `SIMILARITY_THRESHOLD` (float): 기존 노드에 추가하는 유사도 임계값 (기본값: 0.7)
- `EXPLORATION_THRESHOLD` (float): 탐색을 계속하는 유사도 임계값 (기본값: 0.5)

#### 3.1.2 config.json 설정값
**모델 및 성능**:
- `MODEL` (str): 사용할 gemini 모델 (기본값: "gemini-2.5-flash-lite")
- `FANOUT_LIMIT` (int): 부모 노드의 최대 자식 수 (기본값: 5)
- `MAX_SUMMARY_LENGTH` (int): 노드 요약 최대 길이 (기본값: 2000)

**시스템 모드**:
- `SYSTEM_MODE` (str): 실행 모드 ("test" | "chat", 기본값: "chat")
- `SEARCH_MODE` (str): 검색 모드 ("efficiency" | "force" | "no", 기본값: "efficiency")
- `UPDATE_TOPIC` (str): 토픽 업데이트 정책 ("always" | "smart" | "never", 기본값: "never")

**디버그 및 기록**:
- `DEBUG` (bool): 디버그 모드 활성화 여부 (기본값: False)
- `DEBUG_TXT` (bool): 디버그 텍스트 파일 저장 여부 (기본값: False)
- `NO_RECORD` (bool): 대화 기록 비활성화 여부 (기본값: False)

### 3.2 함수 상세 명세

#### 3.2.1 hsms.py 함수
**`show_tree_structure()`**
- **목적**: 계층적 메모리 트리 구조를 ASCII 기반으로 시각화하여 출력
- **기능**: 
  - 전체 노드 수 및 깊이별 통계 제공
  - ASCII 문자(+, -, |)를 사용한 호환성 높은 트리 출력
  - 각 노드의 주제와 포함된 대화 인덱스 표시
- **출력 형식**: 계층적 들여쓰기와 ASCII 아트로 구조 표현

**`get_args()`**
- **목적**: 명령행 인자 파싱 및 검증
- **반환값**: argparse.Namespace 객체
- **기능**: 모든 명령행 옵션 처리, 기본값 설정, 상호 충돌 검사

**`validate_environment()`**
- **목적**: 실행 환경 검증 (API 키, 필수 파일 등)
- **반환값**: bool (검증 성공 여부)

**`initialize_system(args)`**
- **목적**: 인자를 바탕으로 시스템 초기화
- **매개변수**: 파싱된 명령행 인자
- **기능**: config.json 업데이트, 디버그 설정, API 정보 출력

#### 3.2.2 ai_func.py 함수
**기본 AI 호출 함수**:
- `AI(prompt, system, history=None, fine=None, api_key=None, retries=3, debug=False)`
  - **반환값**: str (AI 응답 또는 오류 메시지)
  - **오류 처리**: ResourceExhausted, 일반 예외
  
- `ASYNC_AI(prompt, system, history=None, fine=None, api_key=None, retries=3, debug=False)`
  - **반환값**: str (비동기)
  - **통계 기록**: CALL_STATS 자동 업데이트

- `ASYNC_MULTI_AI(queries, system_prompt, history=None, fine=None, debug=False, start_debug_message="--start", end_debug_message="--end")`
  - **입력**: queries (list), system_prompt (str)
  - **반환값**: list[str] (각 쿼리의 응답)
  - **최적화**: API 키 라운드로빈 분배

**전문 AI 함수**:
- `need_memory_judgement_AI(user_input)`
  - **입력**: user_input (str)
  - **반환값**: bool
  - **판단 기준**: 과거 참조 표현, 요약 요청 등

- `respond_AI(user_input, memory=None)`
  - **입력**: user_input (str), memory (list[int])
  - **반환값**: str (최종 응답)
  - **컨텍스트**: memory 인덱스에 해당하는 과거 대화 포함

- `judgement_similar_multi_AI(node_ids, current_conversation)`
  - **입력**: node_ids (list[str]), current_conversation (str)
  - **반환값**: list[float] (각 노드별 0.0-1.0 점수)
  - **개선사항**: 기존 True/False 판단에서 세밀한 수치 스코어링으로 변경
  - **비동기**: 병렬 처리로 성능 최적화

- `clustering_AI(candidate_node_ids, current_conversation, fanout_limit)`
  - **입력**: candidate_node_ids (list[str]), current_conversation (str), fanout_limit (int)
  - **반환값**: dict {"selected_nodes": list[str], "new_topic": str}
  - **제약**: 같은 부모의 기억 노드들만 대상

**추가 AI 함수**:
- `summary_AI(conversation_data, max_length)`
  - **입력**: conversation_data (dict), max_length (int)
  - **반환값**: str (요약된 텍스트)

- `topic_generation_AI(summary_data)`
  - **입력**: summary_data (str)
  - **반환값**: str (주제명)

- `parent_update_AI(old_summary, max_length)`
  - **입력**: old_summary (str), max_length (int)
  - **반환값**: tuple[str, str] (압축된 요약, 업데이트된 주제명)

#### 3.2.3 main_ai.py 함수
**`command(command_input)`**
- **입력**: command_input (str) - '!'로 시작하는 명령어
- **기능**: 실시간 설정 변경, 상태 확인
- **지원 명령어**: !help, !api-info, !status, !search, !debug, !debug-txt, !fanout-limit, !model, !no-record, !update-topic, !max-summary-length

**`main(user_question)`**
- **입력**: user_question (str)
- **반환값**: str (AI 응답)
- **흐름**: 기억 필요성 판단 → 검색 → 응답 생성 → 저장

**`chat_mode()`**
- **기능**: 터미널 기반 대화형 인터페이스
- **특징**: 무한 루프, 명령어 처리, 예외 처리

**`test_mode()`**
- **기능**: TEST_Q를 사용한 자동화 테스트
- **특징**: 순차 실행, 디버그 정보 출력

#### 3.2.4 tree.py 함수
**검색 관련**:
- `search_tree(current_conversation)`
  - **입력**: current_conversation (str)
  - **반환값**: list[int] (관련 대화의 ALL_MEMORY 인덱스)
  - **알고리즘**: BFS 기반 병렬 유사도 검사

- `find_storage_location(conversation_pair)`
  - **입력**: conversation_pair (dict)
  - **반환값**: dict {"parent_id": str, "existing_memory_node": bool, "node_id": str}
  - **특징**: 단일 경로 탐색, 임계값 기반 결정

**저장 관련**:
- `save_tree(conversation_pair)`
  - **입력**: conversation_pair (dict)
  - **기능**: 대화 저장, 클러스터링, 부모 업데이트 총괄
  - **비동기**: 병렬 처리 최적화

- `perform_clustering(parent_id, new_memory_index, new_conversation_summary)`
  - **입력**: parent_id (str), new_memory_index (int), new_conversation_summary (str)
  - **기능**: 클러스터링 수행, 트리 재구성
  - **제약**: 같은 부모의 기억 노드들만 대상

**노드 관리**:
- `create_new_memory_node(parent_id, memory_index, conversation_summary)`
  - **입력**: parent_id (str), memory_index (int), conversation_summary (str)
  - **반환값**: str (새 노드 ID)

- `add_to_existing_node(node_id, memory_index, conversation_summary)`
  - **입력**: node_id (str), memory_index (int), conversation_summary (str)
  - **기능**: 기존 기억 노드에 대화 추가

- `update_parent_nodes(updated_node_id, new_content_summary)`
  - **입력**: updated_node_id (str), new_content_summary (str)
  - **기능**: 모든 조상 노드의 요약 업데이트
  - **최적화**: 병렬 압축 처리

#### 3.2.5 memory.py 함수
**파일 관리**:
- `save_json(file_path, data, backup=True)`
  - **입력**: file_path (str), data (dict), backup (bool)
  - **안전장치**: 백업 생성, 원자적 쓰기

- `load_json(file_path, default=None)`
  - **입력**: file_path (str), default (any)
  - **반환값**: dict (로드된 데이터 또는 기본값)
  - **복구**: 백업 파일 자동 복구

**데이터 관리**:
- `update_all_memory(new_conversation)`
  - **입력**: new_conversation (dict)
  - **반환값**: int (추가된 대화의 인덱스)

- `get_node_data(node_id)`
  - **입력**: node_id (str)
  - **반환값**: dict (노드 데이터) 또는 None

- `save_node_data(node_id, node_data)`
  - **입력**: node_id (str), node_data (dict)
  - **기능**: 특정 노드 데이터 업데이트

- `create_new_node(topic, summary, parent_id, memory_indexes=None)`
  - **입력**: topic (str), summary (str), parent_id (str), memory_indexes (list[int])
  - **반환값**: str (새 노드 UUID)

#### 3.2.6 config.py 유틸리티 함수
**디버그 관리**:
- `debug_print(message, end='\n')`
  - **입력**: message (str), end (str)
  - **기능**: 타임스탬프 포함 디버그 출력, 파일 저장

- `debug_log_init()`
  - **기능**: 디버그 로그 파일 초기화
  - **파일명**: debug_log_YYYYMMDD_HHMMSS.txt

- `get_timestamp()`
  - **반환값**: str ([YYYY.MM.DD-HH:MM:SS] 형식)

**시스템 유틸리티**:
- `create_uuid()`
  - **반환값**: str (UUID4 문자열)

- `validate_node_structure(node_data)`
  - **입력**: node_data (dict)
  - **반환값**: bool (구조 유효성)

- `get_root_children_ids()`
  - **반환값**: list[str] (ROOT 노드의 직접 자식 ID들)

### 3.3 데이터 구조 및 타입

#### 3.3.1 노드 데이터 구조
```python
NodeData = {
    "node_id": str,              # UUID4 형식
    "topic": str,                # 노드 주제명
    "summary": str,              # 노드 요약
    "direct_parent_id": str,     # 직접 부모 ID (ROOT는 None)
    "all_parent_ids": list[str], # 모든 조상 ID 리스트
    "children_ids": list[str],   # 직접 자식 ID 리스트
    "all_memory_indexes": list[int] # ALL_MEMORY 인덱스 (기억 노드만)
}
```

#### 3.3.2 대화 데이터 구조
```python
ConversationPair = [
    {"role": "user", "content": str},
    {"role": "assistant", "content": str}
]
```

#### 3.3.3 API 호출 통계
```python
CallStats = {
    "total_calls": int,
    "total_time": float,
    "parallel_calls": int,
    "error_count": int,
    "memory_searches": int,
    "cache_hits": int
}
```

## 5. 시스템 개선 히스토리

### 5.1 트리 구조 형성 문제 해결 (2024년 개선)

#### 5.1.1 문제 상황
- **증상**: 노드가 깊이 형성되지 않고 하나의 노드에 편중되는 현상
- **구체적 사례**: "초면 인사" 노드에 서로 다른 주제의 대화 18개가 집중
- **원인 분석**:
  1. AI 유사도 판단이 True/False 기반으로 너무 단순함
  2. 임계값 설정이 부적절하여 구분 능력 부족
  3. 주제 생성 AI의 명명 규칙이 모호함

#### 5.1.2 해결책 구현

**1. 수치 기반 유사도 스코어링 도입**
```python
# 기존: True/False 판단
def judgement_similar_AI(node_summary, current_conversation):
    return "True" or "False"

# 개선: 0.0-1.0 수치 스코어링
def judgement_similar_AI(node_summary, current_conversation):
    return float  # 0.0 (완전 다름) ~ 1.0 (완전 유사)
```

**2. 임계값 최적화**
- `SIMILARITY_THRESHOLD`: 0.7 (기존 노드 추가 기준)
- `EXPLORATION_THRESHOLD`: 0.5 (추가 탐색 여부 기준)
- 세밀한 구분을 통해 적절한 계층 형성 유도

**3. 주제 생성 규칙 강화**
```python
# 개선된 프롬프트 예시
topic_generation_prompt = """
다음 대화들의 공통 주제를 5-8자의 간결한 한국어로 생성하세요.
- 구체적이고 명확한 용어 사용
- 예시: "물리학 개념", "프로그래밍 기초", "일상 대화"
"""
```

#### 5.1.3 개선 결과
- **계층 깊이**: 1-2단계 → 6-7단계로 향상
- **주제 구분**: 모호한 분류 → 명확한 의미적 계층 형성
- **예시 구조**:
  ```
  ROOT
  +-- 정보 확인 및 질의응답
      +-- 과학 정보
          +-- 물리학 주요 개념
              +-- 빛의 속도 [대화 15]
              +-- 중력 법칙 [대화 23, 24]
  ```

### 5.2 시스템 호환성 개선

#### 5.2.1 트리 시각화 호환성
- **문제**: Unicode 문자(├, │, └) 사용으로 일부 환경에서 출력 오류
- **해결**: ASCII 기반 문자(+, -, |) 사용으로 범용 호환성 확보

#### 5.2.2 예외 처리 강화
- AI 함수에서 수치 변환 실패 시 기본값 반환 로직 추가
- 하위 호환성을 위한 fallback 메커니즘 구현


